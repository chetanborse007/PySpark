# Create new input directory on HDFS and copy input files from local
# filesystem to HDFS filesystem.
hadoop fs -rm -r input/WordCount
hadoop fs -mkdir -p input/WordCount
hadoop fs -put input/* input/WordCount


# Create new output directories on HDFS and delete existing output 
# directories generated by previous execution instance of 
# Spark application.
hadoop fs -mkdir -p output/Spark/WordCount
hadoop fs -rm -r output/Spark/WordCount


# Run Spark application on HDFS.
spark-submit --master yarn --deploy-mode client src/WordCount.py -i "input/WordCount" -o "output/Spark/WordCount"


# Get final output from HDFS.
rm -r WordCount/
hadoop fs -get output/Spark/WordCount

